{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para coletar os tweets e os exportar para arquivo Tweets.csv\n",
    "def Coletar_Tweets():\n",
    "    # Criar variável para dia anterior\n",
    "    data = dt.datetime.now()\n",
    "    data = data.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    data = data - dt.timedelta(days=1)\n",
    "\n",
    "    # Criar variável com os termos a serem pesquisados\n",
    "    Termos = \"Bitcoin\"\n",
    "\n",
    "    # Obter Tokens\n",
    "    with open(\"Twitter_API.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        Tokens = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "    print(\"Token Adquirido\")\n",
    "\n",
    "    # Inicializar objeto Client para pesquisa de Tweets\n",
    "    Cliente = tweepy.Client(\n",
    "        bearer_token=Tokens[-1], access_token=Tokens[1], access_token_secret=Tokens[4])\n",
    "\n",
    "    # Criar dicionário cujas keys são as horas do dia\n",
    "    resultados = {}\n",
    "\n",
    "    # Criar listas para segurar os Tweets e as datas correspondentes\n",
    "    Tweets = []\n",
    "    Datas = []\n",
    "\n",
    "    # Recuperar 100 tweets para cada hora do dia anterior e preencher o dicionário\n",
    "    for hora in range(0, 24):\n",
    "        resultados[hora] = Cliente.search_recent_tweets(Termos, max_results=100, tweet_fields=\"created_at\",\n",
    "                                                        start_time=data, end_time=data.replace(hour=data.hour+1))\n",
    "        for i in range(len(resultados[hora].data)):\n",
    "            Datas.append(resultados[hora].data[i].created_at)\n",
    "            Tweets.append(resultados[hora].data[i].text)\n",
    "        data = data.replace(hour=hora)\n",
    "\n",
    "    print(\"Tweets Processados\")\n",
    "\n",
    "    # Criar DataFrame Dados\n",
    "    Df = pd.DataFrame()\n",
    "    Df[\"Tweets\"] = Tweets\n",
    "    Df.index = Datas\n",
    "    Df.sort_index(inplace=True)\n",
    "    # Ler Arquivo existente\n",
    "    Df_Antigo = pd.read_csv(\"Data\\Tweets.csv\", sep=\";\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "    # Adicionar Novas Linhas\n",
    "\n",
    "    Df = pd.concat([Df, Df_Antigo])\n",
    "\n",
    "    # Exportar DataFrame para CSV.\n",
    "    Df.to_csv(\"Data\\Tweets.csv\", sep=\";\")\n",
    "    print(\"Dados exportados com Sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coletar_Tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para coletar os preços de BITCOIN\\USD e exportar paraa Precos.csv\n",
    "def Coletar_Precos():\n",
    "    #Coletar Arquivo de Dados\n",
    "    df = pd.read_csv(\"Data\\Tweets.csv\", sep=\";\", index_col=\"Unnamed: 0\")\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Coletar a data do primeiro tweet coletado\n",
    "    inicio = df.index[0]\n",
    "    inicio = inicio[:10]\n",
    "\n",
    "    # Coletar data do último tweet coletado\n",
    "    fim = df.index[-1]\n",
    "    fim = fim[:10]\n",
    "\n",
    "    print(\"Horizonte temporal adquirido\")\n",
    "\n",
    "    # Obter preços do BTC-USD em intervalos de uma hora desde o primeiro (Somente Preço em que a hora fecha)\n",
    "    Precos = pd.DataFrame()\n",
    "    Precos = yf.Tickers(\"BTC-USD ^GSPC EUR=X ^DJI USO GLD\").history(start=inicio, end=fim, interval=\"1h\")[\"Close\"]]\n",
    "    \n",
    "    Precos.ffill(inplace=True)\n",
    "    # Exportar Preços\n",
    "    Precos.to_csv(\"Data\\Precos.csv\", sep=\";\")\n",
    "    print(\"Dados exportados com sucesso\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizonte temporal adquirido\n",
      "[*********************100%***********************]  6 of 6 completed\n",
      "Dados exportados com sucesso\n"
     ]
    }
   ],
   "source": [
    "Coletar_Precos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar o sentimento de cada tweet coletado e exportar para Sentimento_Agregado.csv\n",
    "def Sentimento():\n",
    "\n",
    "    VADER = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Ler Tweets Coletados\n",
    "    Tweets = pd.read_csv(\"Data\\Tweets.csv\", sep=\";\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "    # Lista para conter os sentimentos agregados\n",
    "    Sentiment = []\n",
    "\n",
    "    # Popular Lista\n",
    "    for i in range(Tweets.shape[0]):\n",
    "        Sentiment.append(VADER.polarity_scores(text=Tweets.iloc[i,0])[\"compound\"])\n",
    "\n",
    "    print(\"Sentimentos adquiridos\")\n",
    "    # Criar Coluna de sentimento no dataframe principal\n",
    "    Tweets[\"Sentimento\"] = Sentiment\n",
    "\n",
    "    # Salvar Dataframe com sentiments\n",
    "    Tweets.to_csv(\"Data\\Sentimento_Agregado.csv\", sep=\";\")\n",
    "\n",
    "    print(\"Dados exportados com Sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimentos adquiridos\n",
      "Dados exportados com Sucesso\n"
     ]
    }
   ],
   "source": [
    "Sentimento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntar o sentimento agregado, tweet original e preço que o BITCOIN fecha na hora para uma único arquivo Base_Completa.csv\n",
    "def Juntar_Base():\n",
    "    \n",
    "    # Ler dados de sentimentos e precos\n",
    "    Precos = pd.read_csv(\"Data\\Precos.csv\", sep=\";\", index_col=\"Unnamed: 0\")\n",
    "    Sentimentos = pd.read_csv(\"Data\\Sentimento_Agregado.csv\", sep=\";\")\n",
    "    \n",
    "    # Converter coluna de horário para datetime, para podermos manipular os valores a serem compatíveis com aqueles\n",
    "    # presentes no dataframe Precos, nos quais minutos e segundos são sempre 0\n",
    "    Sentimentos[\"Unnamed: 0\"] = pd.to_datetime(Sentimentos[\"Unnamed: 0\"])\n",
    "    \n",
    "    # Trocar minutos diferentes de  0 por 0\n",
    "    Sentimentos[\"Unnamed: 0\"] = Sentimentos[\"Unnamed: 0\"].mask(Sentimentos[\"Unnamed: 0\"].dt.minute != 0, Sentimentos[\"Unnamed: 0\"] \\\n",
    "    + pd.offsets.DateOffset(minute=0))    \n",
    "\n",
    "    # Trocar segundos diferentes de 0 por 0\n",
    "    Sentimentos[\"Unnamed: 0\"] = Sentimentos[\"Unnamed: 0\"].mask(Sentimentos[\"Unnamed: 0\"].dt.second != 0, Sentimentos[\"Unnamed: 0\"] \\\n",
    "    + pd.offsets.DateOffset(second=0))\n",
    "    \n",
    "    # Transformar coluna de datas em índice e ordená-la\n",
    "    Sentimentos.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    Sentimentos.sort_index(inplace=True)\n",
    "\n",
    "    # Transformar índice do dataframe de preços em DateTime, para merge funcionar\n",
    "    Precos.index = pd.to_datetime(Precos.index)\n",
    "\n",
    "    # Juntar ambos os DataFrames\n",
    "    Base_Completa = Sentimentos.merge(Precos, left_index=True, right_index=True)\n",
    "    \n",
    "    # Nomear Índice\n",
    "    Base_Completa.index.name = \"Horario\"\n",
    "    \n",
    "    Base_Completa.dropna(inplace=True)\n",
    "    Base_Completa.rename(columns={\"EUR=X\":\"USD/EUR\", \"GLD\":\"Gold_ETF\", \"USO\":\"Oil_ETF\", \"^DJI\":\"Dow_Jones\", \"^GSPC\":\"S&P500\"}, inplace=True)\n",
    "\n",
    "    # Exportar para csv\n",
    "    Base_Completa.to_csv(\"Data\\Base_Completa.csv\", sep=\";\")    \n",
    "\n",
    "    print(\"Dados exportados com Sucesso\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thiago\\miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:760: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados exportados com Sucesso\n"
     ]
    }
   ],
   "source": [
    "Juntar_Base()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f200176434afb1091d5af7be01b9b90418d74b515841510b5081206e46435119"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
